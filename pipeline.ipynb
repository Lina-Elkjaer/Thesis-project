{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeseriesflattener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_female = pd.read_csv('test_data/df_female.csv')\n",
    "df_age = pd.read_csv('test_data/df_age.csv')\n",
    "df_SFI = pd.read_csv('test_data/df_SFI.csv')\n",
    "df_uti = pd.read_csv('test_data/df_uti.csv')\n",
    "df_admissions = pd.read_csv('test_data/df_admissions.csv')\n",
    "\n",
    "#REmoving extra column\n",
    "df_female = df_female.loc[:, ~df_female.columns.str.contains('^Unnamed')]\n",
    "df_age = df_age.loc[:, ~df_age.columns.str.contains('^Unnamed')]\n",
    "df_SFI = df_SFI.loc[:, ~df_SFI.columns.str.contains('^Unnamed')]\n",
    "df_uti = df_uti.loc[:, ~df_uti.columns.str.contains('^Unnamed')]\n",
    "df_admissions = df_admissions.loc[:, ~df_admissions.columns.str.contains('^Unnamed')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_female\n",
    "df_SFI\n",
    "df_uti\n",
    "df_admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating uti df with negative cases as well\n",
    "merged = pd.merge(df_age, df_uti, on=['ID'], how='left')\n",
    "\n",
    "merged = merged[['ID','value']]\n",
    "\n",
    "#turning UVI-neg cases from nan to 0\n",
    "where_are_NaNs = np.isnan(merged)\n",
    "merged[where_are_NaNs] = 0\n",
    "\n",
    "#checking\n",
    "print(merged)\n",
    "\n",
    "#Changing from floats to int\n",
    "merged['value'] = merged['value'].astype('int')\n",
    "\n",
    "#Creating subsets\n",
    "uti_full = merged\n",
    "\n",
    "uti_value = merged['value']\n",
    "print(uti_value)\n",
    "\n",
    "ID = merged['ID']\n",
    "ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ID #The ID column\n",
    "y = uti_value #The outcome column\n",
    "\n",
    "\n",
    "# # Uncomment for a different data split: 60% training, 20% validation, 20% test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "# X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.25, random_state=42) # 0.25 x 0.80 = 0.2\n",
    "\n",
    "\n",
    "#70% training, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify = y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of observations in trainingset:{sum(y_train)}\") #354\n",
    "print(f\"Percentage of positive class in trainingset: {sum(y_train)/len(y_train)*100}\") #27.15% UTI\n",
    "\n",
    "\n",
    "print(f\"Number of observations in testset:{sum(y_test)}\") #152\n",
    "print(f\"Percentage of positive class in testset: {sum(y_test)/len(y_test)*100}\") #27.14% UTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train)\n",
    "\n",
    "#print(len(X_train.unique()))\n",
    "#print(len(ID.unique()))\n",
    "\n",
    "## Creating a df for uti positive within the training set \n",
    "#NB 'value' = uti\n",
    "train_data = pd.DataFrame({'ID' : X_train, 'value' : y_train})\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "train_data_sub = train_data[train_data['value'] == 1]\n",
    "\n",
    "y_train_df = pd.merge(train_data_sub, df_uti, on=['ID', 'value'], how='left')\n",
    "\n",
    "print(len(train_data_sub) == len(y_train_df))\n",
    "\n",
    "print(y_train_df.head())\n",
    "\n",
    "df_uti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a df for uti positive within the test set \n",
    "#NB 'value' = uti\n",
    "test_data = pd.DataFrame({'ID' : X_test, 'value' : y_test})\n",
    "\n",
    "#print(test_data.head())\n",
    "\n",
    "#Subset with only uti-pos\n",
    "test_data_sub = test_data[test_data['value'] == 1]\n",
    "\n",
    "#adding the dates from the uti df\n",
    "y_test_df = pd.merge(test_data_sub, df_uti, on=['ID', 'value'], how='left')\n",
    "\n",
    "#Checking nothing went wrong\n",
    "print(len(test_data_sub) == len(y_test_df))\n",
    "\n",
    "print(y_test_df.head())\n",
    "\n",
    "df_uti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING 12/02\n",
    "\n",
    "X_train_df = pd.DataFrame({'ID' : X_train})\n",
    "\n",
    "predictor_df_dict = {'df_SFI':df_SFI, 'df_age':df_age, 'df_female':df_female}\n",
    "\n",
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING 12/02\n",
    "for key in predictor_df_dict:\n",
    "    d[key] = pd.merge(X_train_df, predictor_df_dict[key], on=['ID'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING 12/02\n",
    "d['df_SFI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_df = pd.DataFrame({'ID' : X_train})\n",
    "\n",
    "X_train_df\n",
    "\n",
    "print(len(df_SFI))\n",
    "\n",
    "X_train_SFI = pd.merge(X_train_df, df_SFI, on=['ID'], how='left')\n",
    "X_train_age = pd.merge(X_train_df, df_age, on=['ID'], how='left')\n",
    "X_train_female = pd.merge(X_train_df, df_female, on=['ID'], how='left')\n",
    "\n",
    "X_train_SFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the different test datasets\n",
    "X_test_df = pd.DataFrame({'ID' : X_test})\n",
    "\n",
    "X_test_df\n",
    "\n",
    "X_test_SFI = pd.merge(X_test_df, df_SFI, on=['ID'], how='left')\n",
    "X_test_age = pd.merge(X_test_df, df_age, on=['ID'], how='left')\n",
    "X_test_female = pd.merge(X_test_df, df_female, on=['ID'], how='left')\n",
    "\n",
    "#Checking everything worked\n",
    "X_test_SFI\n",
    "#print(X_test_age.head())\n",
    "#print(X_test_gender.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a train subset of df_admissions based on the ID's from X_train\n",
    "ID_list_train = list(X_train)\n",
    "\n",
    "ID_list_train\n",
    "\n",
    "df_admissions_train = df_admissions[df_admissions['ID'].isin(ID_list_train)]\n",
    "\n",
    "df_admissions #80876 rows\n",
    "df_admissions_train #56271 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a test subset of df_admissions based on the ID's from X_test\n",
    "ID_list_test = list(X_test)\n",
    "\n",
    "ID_list_test\n",
    "\n",
    "df_admissions_test = df_admissions[df_admissions['ID'].isin(ID_list_test)]\n",
    "\n",
    "df_admissions #80876 rows\n",
    "df_admissions_test #24605 rows #NB lidt mere end 30% fordi der er lidt flere indlæggelsesdage pr patient her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolarr = X_train_SFI['ID'].unique() == X_train_df['ID']\n",
    "\n",
    "print(boolarr.sum() == len(boolarr)) #If true, they are all equal\n",
    "\n",
    "X_train_SFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolarr = X_test_SFI['ID'].unique() == X_test_df['ID']\n",
    "\n",
    "print(boolarr.sum() == len(boolarr)) #If true, they are all equal\n",
    "\n",
    "X_test_SFI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For removing numbers in notes.... Dont know if this should be applied or not....\n",
    "\n",
    "X_train_SFI['note'] = X_train_SFI['note'].str.replace('\\d+', '')\n",
    "X_test_SFI['note'] = X_test_SFI['note'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#sentence_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2') #is this the one I wanna use??\n",
    "\n",
    "sentence_model = SentenceTransformer('encoder-large-v1')\n",
    "sentence_model\n",
    "\n",
    "sentence_embeddings_train = sentence_model.encode(X_train_SFI['note'])\n",
    "sentence_embeddings_test = sentence_model.encode(X_test_SFI['note'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_train_df = pd.DataFrame(sentence_embeddings_train)\n",
    "sentence_embeddings_train_df # 93607 rows × 1024 columns\n",
    "X_train_SFI #93607 rows × 3 columns\n",
    "#X_train_df\n",
    "\n",
    "sentence_embeddings_test_df = pd.DataFrame(sentence_embeddings_test)\n",
    "sentence_embeddings_test_df # 39478 rows × 1024 columns\n",
    "X_test_SFI #39478 rows × 3 columns              #Bare lige et tjek for at de rent faktisk er samme størrelse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = pd.merge(X_train_SFI, sentence_embeddings_train_df, how='left', left_index=True, right_index=True)\n",
    "X_train_embeddings \n",
    "\n",
    "df_sentence_train = X_train_embeddings.loc[:, X_train_embeddings.columns != 'note']\n",
    "\n",
    "df_sentence_train \n",
    "\n",
    "\n",
    "\n",
    "X_test_embeddings = pd.merge(X_test_SFI, sentence_embeddings_test_df, how='left', left_index=True, right_index=True)\n",
    "X_test_embeddings \n",
    "\n",
    "df_sentence_test = X_test_embeddings.loc[:, X_test_embeddings.columns != 'note']\n",
    "\n",
    "df_sentence_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tester cosine similarity, bare for at se om embeddingsene er okay\n",
    "\n",
    "# for sentence, embedding in zip(X_train_SFI['note'], sentence_embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# cos_sim1 = util.cos_sim(sentence_embeddings[93603], sentence_embeddings[93605])\n",
    "# print(\"Cosine-Similarity:\", cos_sim1) #Similar sentences (våd ble, uridom påsat     Våd ble skiftet, uridom på) cosine = 0.9\n",
    "\n",
    "# cos_sim2 = util.cos_sim(sentence_embeddings[3], sentence_embeddings[93605])\n",
    "# print(\"Cosine-Similarity:\", cos_sim2) #UNsimilar sentences (ingen lugt    Våd ble skiftet, uridom på) cosine = 0.68\n",
    "\n",
    "\n",
    "# #X_train_embeddings = pd.merge(X_train_SFI, sentence_embeddings_df, how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making tf-idf-data ready for flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseriesflattener.df_transforms import (\n",
    "    df_with_multiple_values_to_named_dataframes,\n",
    ")\n",
    "\n",
    "# split the dataframe into a list of named dataframes with one value each\n",
    "embedded_dfs_train = df_with_multiple_values_to_named_dataframes(\n",
    "    df=df_sentence_train,\n",
    "    entity_id_col_name=\"ID\",\n",
    "    timestamp_col_name=\"date\",\n",
    "    name_prefix=\"tfidf_\",\n",
    ")\n",
    "\n",
    "# check the first dataframe\n",
    "embedded_dfs_train[0].df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseriesflattener.df_transforms import (\n",
    "    df_with_multiple_values_to_named_dataframes,\n",
    ")\n",
    "\n",
    "# split the dataframe into a list of named dataframes with one value each\n",
    "embedded_dfs_test = df_with_multiple_values_to_named_dataframes(\n",
    "    df=df_sentence_test,\n",
    "    entity_id_col_name=\"ID\",\n",
    "    timestamp_col_name=\"date\",\n",
    "    name_prefix=\"tfidf_\",\n",
    ")\n",
    "\n",
    "# check the first dataframe\n",
    "embedded_dfs_test[0].df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseriesflattener.aggregation_fns import mean\n",
    "from timeseriesflattener.feature_specs.group_specs import PredictorGroupSpec\n",
    "import numpy as np\n",
    "\n",
    "# create a group spec for the embedded text that will take the mean of each embedding on the column axis\n",
    "# for the last 365 and 730 days\n",
    "emb_spec_batch_train = PredictorGroupSpec(\n",
    "    named_dataframes=embedded_dfs_train,\n",
    "    lookbehind_days=[4], #kan tilføje flere [7, 14] så for man en predictor som kigger 7 dage tilbage og en der kigger 14 dage tilbage\n",
    "    fallback=[np.nan],\n",
    "    aggregation_fns=[mean],\n",
    ").create_combinations()\n",
    "\n",
    "# print the number of features we will create\n",
    "print(len(emb_spec_batch_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseriesflattener.aggregation_fns import mean\n",
    "from timeseriesflattener.feature_specs.group_specs import PredictorGroupSpec\n",
    "import numpy as np\n",
    "\n",
    "# create a group spec for the embedded text that will take the mean of each embedding on the column axis\n",
    "# for the last 365 and 730 days\n",
    "emb_spec_batch_test = PredictorGroupSpec(\n",
    "    named_dataframes=embedded_dfs_test,\n",
    "    lookbehind_days=[4], #kan tilføje flere [7, 14] så for man en predictor som kigger 7 dage tilbage og en der kigger 14 dage tilbage\n",
    "    fallback=[np.nan],\n",
    "    aggregation_fns=[mean],\n",
    ").create_combinations()\n",
    "\n",
    "# print the number of features we will create\n",
    "print(len(emb_spec_batch_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions_train2 = df_admissions_train.reset_index(drop=True)\n",
    "df_admissions_train2 \n",
    "df_admissions_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions_test2 = df_admissions_test.reset_index(drop=True)\n",
    "\n",
    "df_admissions_test2 \n",
    "df_admissions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseriesflattener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load a dataframe with times you wish to make a prediction\n",
    "    prediction_times_df = df_admissions_train2\n",
    "    # Load a dataframe with raw values you wish to aggregate as predictors\n",
    "    #predictor_df =  # Using my embeddings above\n",
    "    # Load a dataframe specifying when the outcome occurs\n",
    "    outcome_df = y_train_df\n",
    "\n",
    "    # Specify how to aggregate the predictors and define the outcome\n",
    "    from timeseriesflattener.feature_specs.single_specs import OutcomeSpec, PredictorSpec, StaticSpec\n",
    "    from timeseriesflattener.aggregation_fns import maximum, mean\n",
    "\n",
    "    sex_predictor_spec_train = StaticSpec(\n",
    "       timeseries_df= X_train_female,\n",
    "       feature_base_name=\"female\",\n",
    "       prefix=\"pred\",\n",
    "     )\n",
    "    \n",
    "    age_predictor_spec_train = StaticSpec(\n",
    "       timeseries_df= X_train_age,\n",
    "       feature_base_name=\"age\",\n",
    "       prefix=\"pred\",\n",
    "     )\n",
    "\n",
    "    outcome_spec_train = OutcomeSpec(\n",
    "        timeseries_df=outcome_df,\n",
    "        lookahead_days=3,\n",
    "        fallback=0,\n",
    "        aggregation_fn=maximum,\n",
    "        feature_base_name=\"UTI\",\n",
    "        incident=False,\n",
    "    )\n",
    "\n",
    "    # Instantiate TimeseriesFlattener and add the specifications\n",
    "    from timeseriesflattener import TimeseriesFlattener\n",
    "\n",
    "    ts_flattener_train = TimeseriesFlattener(\n",
    "        prediction_times_df=prediction_times_df,\n",
    "        entity_id_col_name=\"ID\",\n",
    "        timestamp_col_name=\"date\",\n",
    "        n_workers=40,\n",
    "        drop_pred_times_with_insufficient_look_distance=True,\n",
    "    )\n",
    "    ts_flattener_train.add_spec([sex_predictor_spec_train, outcome_spec_train, *emb_spec_batch_train, age_predictor_spec_train]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "    df_train = ts_flattener_train.get_df()\n",
    "    df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding additional data\n",
    "\n",
    "df_FIM_total = pd.read_csv('test_data2/df_FIM_total.csv')\n",
    "df_acute_days = pd.read_csv('test_data2/df_acute_days.csv')\n",
    "df_brain_injury = pd.read_csv('test_data2/df_brain_injury_mod.csv')\n",
    "\n",
    "df_FIM_total = df_FIM_total.loc[:, ~df_FIM_total.columns.str.contains('^Unnamed')]\n",
    "df_acute_days = df_acute_days.loc[:, ~df_acute_days.columns.str.contains('^Unnamed')]\n",
    "df_brain_injury = df_brain_injury.loc[:, ~df_brain_injury.columns.str.contains('^Unnamed')]\n",
    "\n",
    "X_train_brain_injury = pd.merge(X_train_df, df_brain_injury, on=['ID'], how='left')\n",
    "X_train_acute_days = pd.merge(X_train_df, df_acute_days, on=['ID'], how='left')\n",
    "X_train_FIM_total = pd.merge(X_train_df, df_FIM_total, on=['ID'], how='left')\n",
    "\n",
    "\n",
    "# X_train_acute_days[' acute_days'] = X_train_acute_days[' acute_days'].astype(int)\n",
    "# print(X_train_brain_injury)\n",
    "# print(X_train_acute_days)\n",
    "# print(X_train_FIM_total)\n",
    "# print(X_train_acute_days.dtypes)\n",
    "# print(np.unique(X_train_acute_days[' acute_days']))\n",
    "\n",
    "X_test_brain_injury = pd.merge(X_test_df, df_brain_injury, on=['ID'], how='left')\n",
    "X_test_acute_days = pd.merge(X_test_df, df_acute_days, on=['ID'], how='left')\n",
    "X_test_FIM_total = pd.merge(X_test_df, df_FIM_total, on=['ID'], how='left')\n",
    "\n",
    "\n",
    "#rename FIM_total to value (column-name needs to be value for timeseriesflattener)\n",
    "X_train_FIM_total = X_train_FIM_total.rename(columns= {'FIM_total': 'value'})\n",
    "\n",
    "#rename FIM_total to value (column-name needs to be value for timeseriesflattener)\n",
    "X_test_FIM_total = X_test_FIM_total.rename(columns= {'FIM_total': 'value'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acute_days_predictor_spec_train = StaticSpec(\n",
    "   timeseries_df= X_train_acute_days,\n",
    "   feature_base_name=\"acute_days\",\n",
    "   prefix=\"pred\" )\n",
    "\n",
    "\n",
    "ts_flattener_train.add_spec([acute_days_predictor_spec_train]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "df_train = ts_flattener_train.get_df()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "brain_injury_predictor_spec_train = StaticSpec(\n",
    "   timeseries_df= X_train_brain_injury,\n",
    "   feature_base_name=\"brain_injury\",\n",
    "   prefix=\"pred\")\n",
    "\n",
    "\n",
    "ts_flattener_train.add_spec([brain_injury_predictor_spec_train ]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "df_train = ts_flattener_train.get_df()\n",
    "df_train\n",
    "    \n",
    "# FIM_total_predictor_spec_train = PredictorSpec(\n",
    "#    timeseries_df= X_train_FIM_total,\n",
    "#    lookbehind_days=4,\n",
    "#    fallback=np.nan, \n",
    "#    aggregation_fn=mean,\n",
    "#    feature_base_name=\"FIM_total\") #prefix=\"pred\" Sker automatisk fordi det er en PredictorSpec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FIM_total_predictor_spec_train = PredictorSpec(\n",
    "    timeseries_df= X_train_FIM_total,\n",
    "    lookbehind_days=4,\n",
    "    fallback=np.nan, \n",
    "    aggregation_fn=mean,\n",
    "    feature_base_name=\"FIM_total\") #prefix=\"pred\" Sker automatisk fordi det er en PredictorSpec\n",
    "\n",
    "ts_flattener_train.add_spec([FIM_total_predictor_spec_train]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "df_train = ts_flattener_train.get_df()\n",
    "df_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_csv(\"test_data/testing_df_X_train.csv\")\n",
    "\n",
    "df_train.dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load a dataframe with times you wish to make a prediction\n",
    "    prediction_times_df = df_admissions_test2\n",
    "    # Load a dataframe with raw values you wish to aggregate as predictors\n",
    "    #predictor_df =  # Using my embeddings above\n",
    "    # Load a dataframe specifying when the outcome occurs\n",
    "    outcome_df = y_test_df\n",
    "\n",
    "    # Specify how to aggregate the predictors and define the outcome\n",
    "    from timeseriesflattener.feature_specs.single_specs import OutcomeSpec, PredictorSpec, StaticSpec\n",
    "    from timeseriesflattener.aggregation_fns import maximum, mean\n",
    "\n",
    "    sex_predictor_spec_test = StaticSpec(\n",
    "       timeseries_df= X_test_female,\n",
    "       feature_base_name=\"female\",\n",
    "       prefix=\"pred\",\n",
    "     )\n",
    "    \n",
    "    age_predictor_spec_test = StaticSpec(\n",
    "       timeseries_df= X_test_age,\n",
    "       feature_base_name=\"age\",\n",
    "       prefix=\"pred\",\n",
    "     )\n",
    "\n",
    "    outcome_spec_test = OutcomeSpec(\n",
    "        timeseries_df=outcome_df,\n",
    "        lookahead_days=3,\n",
    "        fallback=0,\n",
    "        aggregation_fn=maximum,\n",
    "        feature_base_name=\"UTI\",\n",
    "        incident=False,\n",
    "    )\n",
    "\n",
    "    # Instantiate TimeseriesFlattener and add the specifications\n",
    "    from timeseriesflattener import TimeseriesFlattener\n",
    "\n",
    "    ts_flattener_test = TimeseriesFlattener(\n",
    "        prediction_times_df=prediction_times_df,\n",
    "        entity_id_col_name=\"ID\",\n",
    "        timestamp_col_name=\"date\",\n",
    "        n_workers=40,\n",
    "        drop_pred_times_with_insufficient_look_distance=True,\n",
    "    )\n",
    "    ts_flattener_test.add_spec([sex_predictor_spec_test, outcome_spec_test, *emb_spec_batch_test, age_predictor_spec_test]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "    df_test = ts_flattener_test.get_df()\n",
    "    df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acute_days_predictor_spec_test = StaticSpec(\n",
    "   timeseries_df= X_test_acute_days,\n",
    "   feature_base_name=\"acute_days\",\n",
    "   prefix=\"pred\" )\n",
    "\n",
    "\n",
    "ts_flattener_test.add_spec([acute_days_predictor_spec_test]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "df_test = ts_flattener_test.get_df()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_injury_predictor_spec_test = StaticSpec(\n",
    "   timeseries_df= X_test_brain_injury,\n",
    "   feature_base_name=\"brain_injury\",\n",
    "   prefix=\"pred\")\n",
    "\n",
    "\n",
    "ts_flattener_test.add_spec([brain_injury_predictor_spec_test ]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "df_test = ts_flattener_test.get_df()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIM_total_predictor_spec_test= PredictorSpec(\n",
    "    timeseries_df= X_test_FIM_total,\n",
    "    lookbehind_days=4,\n",
    "    fallback=np.nan, \n",
    "    aggregation_fn=mean,\n",
    "    feature_base_name=\"FIM_total\") #prefix=\"pred\" Sker automatisk fordi det er en PredictorSpec\n",
    "\n",
    "ts_flattener_test.add_spec([FIM_total_predictor_spec_test]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "df_test = ts_flattener_test.get_df()\n",
    "df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Added 13/02. ADDING PSYK_group SFI DATA (1/2) NB:tfidf embeddings\n",
    "\n",
    "# #Model is not better with this data, but it is far slower\n",
    "\n",
    "\n",
    "# df_SFI_psyk = pd.read_csv('test_data2/df_SFI_psyk.csv')\n",
    "\n",
    "# df_SFI_psyk = df_SFI_psyk.loc[:, ~df_SFI_psyk.columns.str.contains('^Unnamed')]\n",
    "\n",
    "\n",
    "# X_train_SFI_psyk = pd.merge(X_train_df, df_SFI_psyk, on=['ID'], how='left')\n",
    "# X_test_SFI_psyk = pd.merge(X_test_df, df_SFI_psyk, on=['ID'], how='left')\n",
    "\n",
    "# X_train_SFI_psyk = X_train_SFI_psyk.dropna()\n",
    "\n",
    "# X_test_SFI_psyk = X_test_SFI_psyk.dropna()\n",
    "\n",
    "\n",
    "# X_train_SFI_psyk['note'] = X_train_SFI_psyk['note'].str.replace('\\d+', '')\n",
    "# X_test_SFI_psyk['note'] = X_test_SFI_psyk['note'].str.replace('\\d+', '')\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from spacy.lang.da.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "# danish_stopwords = list(STOP_WORDS)\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words = danish_stopwords, min_df = 5, max_features=200) #with min_df = 2, 7676 tokens. without 11314 tokens, with min_df = 5, 4031 tokens, min_df = 100. 622 tokens\n",
    "# #Really bad results with min_df = 5 and max_features = 200, i.e. highest of cross-val-score was about 0.65, lowest about 0.35\n",
    "# #same for min_df = 5 and max_features = 500\n",
    "# #same for min_df = 5 and max_features = 1000 \n",
    "# #same for min_df = 20 and max_features = 500\n",
    "\n",
    "# X_train_psyk = vectorizer.fit_transform(X_train_SFI_psyk['note'])\n",
    "\n",
    "# #print(\"Tokens\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# tokens_psyk = vectorizer.get_feature_names_out()\n",
    "\n",
    "# print(X_train_psyk.shape) \n",
    "\n",
    "# X_train_psyk\n",
    "# # Convert sparse matrix to DataFrame\n",
    "# df_tfidfvect_train_psyk = pd.DataFrame(data = X_train_psyk.toarray(), columns = tokens_psyk)\n",
    "# df_tfidfvect_train_psyk\n",
    "\n",
    "\n",
    "# #Running the model on the test dataset NB using transform() instead of fit transform, in order to avoid training the model on the  test set\n",
    "# X_test_psyk = vectorizer.transform(X_test_SFI_psyk['note'])\n",
    "\n",
    "# ##### DETTE SKAL VEL IKKE VÆRE I TEST PIPELINEN?\n",
    "# print(\"Tokens\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# tokens_psyk = vectorizer.get_feature_names_out()\n",
    "\n",
    "# print(X_test_psyk.shape) \n",
    "\n",
    "# X_test_psyk\n",
    "# # Convert sparse matrix to DataFrame\n",
    "# df_tfidfvect_test_psyk = pd.DataFrame(data = X_test_psyk.toarray(), columns = tokens_psyk) #bruger tokens fra X_train\n",
    "# df_tfidfvect_test_psyk\n",
    "\n",
    "# #Adding date and ID to the train df\n",
    "# df_tfidfvect_train_psyk['ID'] = X_train_SFI_psyk['ID']\n",
    "# df_tfidfvect_train_psyk['date'] = X_train_SFI_psyk['date']\n",
    "\n",
    "# #Checking \n",
    "# df_tfidfvect_train_psyk\n",
    "\n",
    "# #Adding date and ID to the train df\n",
    "# df_tfidfvect_test_psyk['ID'] = X_test_SFI_psyk['ID']\n",
    "# df_tfidfvect_test_psyk['date'] = X_test_SFI_psyk['date']\n",
    "\n",
    "# #Checking \n",
    "# df_tfidfvect_test_psyk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Added 13/02 ADDING PSYK_group SFI DATA (2/2) NB:tfidf embeddings\n",
    "\n",
    "# from timeseriesflattener.df_transforms import (\n",
    "#     df_with_multiple_values_to_named_dataframes,\n",
    "# )\n",
    "\n",
    "# # split the dataframe into a list of named dataframes with one value each\n",
    "# embedded_dfs_train_psyk = df_with_multiple_values_to_named_dataframes(\n",
    "#     df=df_tfidfvect_train_psyk,\n",
    "#     entity_id_col_name=\"ID\",\n",
    "#     timestamp_col_name=\"date\",\n",
    "#     name_prefix=\"tfidf_\",\n",
    "# )\n",
    "\n",
    "# # check the first dataframe\n",
    "# embedded_dfs_train_psyk[0].df.head()\n",
    "\n",
    "\n",
    "# # split the dataframe into a list of named dataframes with one value each\n",
    "# embedded_dfs_test_psyk = df_with_multiple_values_to_named_dataframes(\n",
    "#     df=df_tfidfvect_test_psyk,\n",
    "#     entity_id_col_name=\"ID\",\n",
    "#     timestamp_col_name=\"date\",\n",
    "#     name_prefix=\"tfidf_\",\n",
    "# )\n",
    "\n",
    "# # check the first dataframe\n",
    "# embedded_dfs_test_psyk[0].df.head()\n",
    "\n",
    "\n",
    "# from timeseriesflattener.aggregation_fns import mean\n",
    "# from timeseriesflattener.feature_specs.group_specs import PredictorGroupSpec\n",
    "# import numpy as np\n",
    "\n",
    "# # create a group spec for the embedded text that will take the mean of each embedding on the column axis\n",
    "# # for the last 365 and 730 days\n",
    "# emb_spec_batch_train_psyk = PredictorGroupSpec(\n",
    "#     named_dataframes=embedded_dfs_train_psyk,\n",
    "#     lookbehind_days=[4], #kan tilføje flere [7, 14] så for man en predictor som kigger 7 dage tilbage og en der kigger 14 dage tilbage\n",
    "#     fallback=[np.nan],\n",
    "#     aggregation_fns=[mean],\n",
    "# ).create_combinations()\n",
    "\n",
    "# # print the number of features we will create\n",
    "# print(len(emb_spec_batch_train_psyk))\n",
    "\n",
    "\n",
    "\n",
    "# # create a group spec for the embedded text that will take the mean of each embedding on the column axis\n",
    "# # for the last 365 and 730 days\n",
    "# emb_spec_batch_test_psyk = PredictorGroupSpec(\n",
    "#     named_dataframes=embedded_dfs_test_psyk,\n",
    "#     lookbehind_days=[4], #kan tilføje flere [7, 14] så for man en predictor som kigger 7 dage tilbage og en der kigger 14 dage tilbage\n",
    "#     fallback=[np.nan],\n",
    "#     aggregation_fns=[mean],\n",
    "# ).create_combinations()\n",
    "\n",
    "# # print the number of features we will create\n",
    "# print(len(emb_spec_batch_test_psyk))\n",
    "\n",
    "\n",
    "# ts_flattener_train.add_spec([*emb_spec_batch_train_psyk]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "# df_train = ts_flattener_train.get_df()\n",
    "# df_train\n",
    "\n",
    "\n",
    "# ts_flattener_test.add_spec([*emb_spec_batch_test_psyk]) #Stjerne for at unpack listen, yderligere predictors tilføjes blot til listen\n",
    "    \n",
    "# df_test = ts_flattener_test.get_df()\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT STEPS\n",
    "\n",
    "impute (måde og handle missing), prøv xgboost (som selv dealer med missing)\n",
    "\n",
    "fjern dublicates i dataset, check\n",
    "\n",
    "evt merge nye specs på ved prediction_time_uuid, check\n",
    "\n",
    "sklearn crossvalidator_score(), check\n",
    "\n",
    "Lav admissions_df hvor admission times er i hhv train og test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.to_csv(\"test_data/testing_df_X_test.csv\")\n",
    "\n",
    "df_test.dropna().head()\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "#df= timeseriesflattenener.get_df([specs]) # koden fra sidst\n",
    "\n",
    "xgb_model = XGBClassifier(objective=\"binary:logistic\", random_state=42, enable_categorical = True, tree_method = 'hist')\n",
    "#xgb_model = XGBClassifier(objective=\"binary:logistic\", random_state=42, enable_categorical = True, tree_method = 'hist', reg_lambda = 10, reg_alpha = 2) #Virker fint, men gør ikke rigtig nogen forskel\n",
    "#xgb_model = XGBClassifier(objective=\"binary:logistic\", class_weight = 'balanced', random_state=42, enable_categorical = True, tree_method = 'hist')\n",
    "\n",
    "# find alle predictor columns (har præfix \"pred_\")\n",
    "predictor_columns_train =  [col for col in df_train if col.startswith('pred_')]\n",
    "\n",
    "# find outcome column (har præfix \"outc_\")\n",
    "outcome_column_train = [col for col in df_train if col.startswith('outc_')]\n",
    "\n",
    "# find metadata columns (fx prediction_time_uuid)\n",
    "metadata_columns_train = [col for col in df_train if col not in predictor_columns_train + outcome_column_train]\n",
    "\n",
    "\n",
    "# lav X der kun er predictors, og y der kun er outcome\n",
    "X_final_train = df_train[predictor_columns_train]\n",
    "y_final_train = df_train[outcome_column_train]\n",
    "\n",
    "X_final_train.to_csv(\"test_data/X_final_train1.csv\") \n",
    "y_final_train.to_csv(\"test_data/y_final_train1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find alle predictor columns (har præfix \"pred_\")\n",
    "predictor_columns_test =  [col for col in df_test if col.startswith('pred_')]\n",
    "\n",
    "# find outcome column (har præfix \"outc_\")\n",
    "outcome_column_test = [col for col in df_test if col.startswith('outc_')]\n",
    "\n",
    "# find metadata columns (fx prediction_time_uuid)\n",
    "metadata_columns_test = [col for col in df_test if col not in predictor_columns_test + outcome_column_test]\n",
    "\n",
    "\n",
    "# lav X der kun er predictors, og y der kun er outcome\n",
    "X_final_test = df_test[predictor_columns_test]\n",
    "y_final_test = df_test[outcome_column_test]\n",
    "\n",
    "X_final_test.to_csv(\"test_data/X_final_test1.csv\")\n",
    "y_final_test.to_csv(\"test_data/y_final_test1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here to avoid rerunning everything after crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "#df= timeseriesflattenener.get_df([specs]) # koden fra sidst. NB stavet forkert, hvis det ikke virker er det nok grunden\n",
    "\n",
    "#xgb_model = XGBClassifier(objective=\"binary:logistic\", random_state=42, enable_categorical = True, tree_method = 'hist')\n",
    "xgb_model = XGBClassifier(objective=\"binary:logistic\", random_state=42, enable_categorical = True, tree_method = 'hist', reg_alpha = 20)\n",
    "#xgb_model = XGBClassifier(objective=\"binary:logistic\", random_state=42, enable_categorical = True, tree_method = 'hist', reg_lambda = 100) #, reg_alpha = 30) #Virker fint, men gør ikke rigtig nogen forskel\n",
    "\n",
    "\n",
    "X_final_train = pd.read_csv('test_data/X_final_train1.csv') #X_final_train = med tal i tf-idf, X_final_train1 = ingen tal i tf-idf\n",
    "y_final_train = pd.read_csv('test_data/y_final_train1.csv')\n",
    "\n",
    "X_final_test = pd.read_csv('test_data/X_final_test1.csv')\n",
    "y_final_test = pd.read_csv('test_data/y_final_test1.csv')\n",
    "\n",
    "#REmoving extra column\n",
    "X_final_train = X_final_train.loc[:, ~X_final_train.columns.str.contains('^Unnamed')]\n",
    "y_final_train = y_final_train.loc[:, ~y_final_train.columns.str.contains('^Unnamed')]\n",
    "X_final_test = X_final_test.loc[:, ~X_final_test.columns.str.contains('^Unnamed')]\n",
    "y_final_test = y_final_test.loc[:, ~y_final_test.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling \n",
    "from imblearn.over_sampling import RandomOverSampler #, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler #,ClusterCentroids\n",
    "\n",
    "ros = RandomOverSampler(random_state=42) #, shrinkage = 0.5) #Can't use shrinkage. When shrinkage is used, X needs to contain only numerical data to later generate a smoothed bootstrap sample.\n",
    "rus = RandomUnderSampler(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERSAMPLING\n",
    "X_final_train_resampled, y_final_train_resampled = ros.fit_resample(X_final_train, y_final_train)\n",
    "\n",
    "#Produces error, SMOTE and ADASYN cannot handle missing values\n",
    "#X_final_train_resampled, y_final_train_resampled = SMOTE().fit_resample(X_final_train, y_final_train)\n",
    "#X_final_train_resampled, y_final_train_resampled = ADASYN().fit_resample(X_final_train, y_final_train)\n",
    "## Suggests to look at impute https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html to deal with the missing values\n",
    "\n",
    "\n",
    "\n",
    "#UNDERSAMPLING\n",
    "#X_final_train_resampled, y_final_train_resampled = rus.fit_resample(X_final_train, y_final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_final_train[:-1].sum()/len(y_final_train[:-1])*100) #0.9293% UTI after timeseriesflattener\n",
    "print(y_final_train[:-1].sum()) #835 UTI\n",
    "print(len(y_final_train[:-1]))  #89853 TOTAL\n",
    "\n",
    "\n",
    "print(y_final_test[:-1].sum()/len(y_final_test[:-1])*100) #0.9548% UTI after timeseriesflattener\n",
    "print(y_final_test[:-1].sum()) #378 UTI\n",
    "print(len(y_final_test[:-1]))  #39591 TOTAL\n",
    "\n",
    "#Resampled\n",
    "print(y_final_train_resampled[:-1].sum()/len(y_final_train_resampled[:-1])*100) #50% UTI after resampling\n",
    "print(y_final_train_resampled[:-1].sum()) #89018 UTI\n",
    "print(len(y_final_train_resampled[:-1]))  #178037 TOTAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final_train['outc_UTI_within_3_days_maximum_fallback_0_dichotomous']\n",
    "y_final_train_resampled['outc_UTI_within_3_days_maximum_fallback_0_dichotomous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class weights\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "y_true_train = []\n",
    "\n",
    "for i in y_final_train['outc_UTI_within_3_days_maximum_fallback_0_dichotomous']:\n",
    "    y_true_train.append(int(i))\n",
    "\n",
    "weight_dict = {0:1, 1:100}\n",
    "\n",
    "class_weights2 = class_weight.compute_sample_weight(class_weight='balanced', y = y_final_train['outc_UTI_within_3_days_maximum_fallback_0_dichotomous'])\n",
    "class_weights3 = class_weight.compute_sample_weight(class_weight='balanced', y = y_final_train_resampled['outc_UTI_within_3_days_maximum_fallback_0_dichotomous'])\n",
    "#class_weights2 = class_weight.compute_sample_weight(class_weight= weight_dict, y = y_true_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(*class_weights2, sep=\",\")\n",
    "\n",
    "print(np.unique(class_weights2))\n",
    "print(np.unique(class_weights3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_train[[\"pred_brain_injury\"]] = X_final_train[[\"pred_brain_injury\"]].astype(\"category\")\n",
    "X_final_test[[\"pred_brain_injury\"]] = X_final_test[[\"pred_brain_injury\"]].astype(\"category\")\n",
    "print(X_final_train.dtypes)\n",
    "print(X_final_test.dtypes)\n",
    "\n",
    "X_final_train_resampled[[\"pred_brain_injury\"]] = X_final_train_resampled[[\"pred_brain_injury\"]].astype(\"category\")\n",
    "print(X_final_train_resampled.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(objective=\"binary:logistic\", random_state=42, enable_categorical = True, tree_method = 'hist', reg_alpha = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fit!\n",
    "#xgb_model.fit(X_final_train, y_final_train)\n",
    "xgb_model.fit(X_final_train, y_final_train, sample_weight = class_weights2)\n",
    "\n",
    "#Resampled\n",
    "#xgb_model.fit(X_final_train_resampled, y_final_train_resampled, sample_weight = class_weights3)\n",
    "\n",
    "# Evaluate\n",
    "#print(cross_val_score(xgb_model, X_final_train, y_final_train, cv = 5, scoring = 'roc_auc'))\n",
    "\n",
    "print(cross_val_score(xgb_model, X_final_train_resampled, y_final_train_resampled, cv = 5, scoring = 'roc_auc'))\n",
    "\n",
    "#Reorder columns so they are in the correct order, otherwise I get an error \n",
    "#List of column names\n",
    "cols_when_model_builds = xgb_model.get_booster().feature_names\n",
    "\n",
    "#Reordering the columns of the dataframe \n",
    "X_final_test = X_final_test[cols_when_model_builds]\n",
    "\n",
    "\n",
    "#Testing on actual new data\n",
    "\n",
    "# make predictions\n",
    "y_pred = xgb_model.predict(X_final_test)  \n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_final_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#Accuracy: 99.05% becuase it predicts 0 every time......\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TJEK OM JEG HAR CLASS 6 OPGAVEN FRA NLP-KURSUS\n",
    "#Har jeg ikke, slettet fra U-cloud og lå aldrig lokalt :( evt spørg Anna eller en anden fra årgangen under???\n",
    "\n",
    "from LSTM import RNN\n",
    "from torch import nn\n",
    "\n",
    "embeddings = df_sentence_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FloatTensor containing pretrained weights\n",
    "#weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "#embedding = nn.Embedding.from_pretrained(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Måske Ross' LSTM ikke er god, lader til at den vil lave embeddings selv... Jeg skal jo bare bruge en der kan læse det df jeg allerede har...\n",
    "#embeddings = nn.Embedding.from_pretrained(X_final_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = RNN(3, embeddings, 2)  #Youtube tutorial calls output_dim => input_len, hidden_dim_size => hidden_size. ALso has no embedding and no fc, but has \"output_layer\".  NB has input_len = 28 and sequence_len = 28, maybe corresponds to pixels in an image (mNist)\n",
    "\n",
    "# print(LSTM_model) \n",
    "\n",
    "# #LSTM_model = RNN(output_dim: int, embedding_layer: nn.Embedding, hidden_dim_size: int)\n",
    "# #What is output_dim? Is it the amount of predictions, or maybe the amount of possible prediction values????\n",
    "# #Embedding layer seems like saved word_embeddings from earlier, do I need to seperate them from the other predictors and save as an nn.Embedding object?? \n",
    "# # hidden_dim_size? The dimension of the hidden layer? Or the amount of hidden layers?\n",
    "\n",
    "# #Might need to include an optimizer in LSTM.py.... right now it will not optimize weights and biases. I actually cannot see the weights and biases in Ross' code, so I am a bit confused.¨\n",
    "# # 10:48 ---> https://www.youtube.com/watch?v=FHdlXe1bSe4&t=840s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {cross_val_score(xgb_model, X_final_train_resampled, y_final_train_resampled, cv = 5, scoring = \"accuracy\")}')\n",
    "print(f'ROC-AUC: {cross_val_score(xgb_model, X_final_train_resampled, y_final_train_resampled, cv = 5, scoring = \"roc_auc\")}')\n",
    "print(f'F1-score: {cross_val_score(xgb_model, X_final_train_resampled, y_final_train_resampled, cv = 5, scoring = \"f1\")}') #Should be possible to change the weights of f1, so it favors recall over precision. Would be relevant in my case as it is more important to have fewer false-negatives, than false-positives (more worried about MISSING true cases than mis-diagnosing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(f\"True Positives: {TP}. False Positives: {FP}. True negatives: {TN}. False negatives: {FN}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "\n",
    "for i in y_final_test['outc_UTI_within_3_days_maximum_fallback_0_dichotomous']:\n",
    "    y_true.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perf_measure(y_true, predictions))\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(f'Precision = {average_precision_score(y_true, predictions)}')\n",
    "print(f'Recall = {recall_score(y_true, predictions)}')\n",
    "print(f'F1 score = {f1_score(y_true, predictions)}')\n",
    "print(f'ROC-AUC score = {roc_auc_score(y_true, predictions)}') #0.524 that is predictive ability is not better than random guessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    xgb_model, X_final_test, y_final_test, name=\"XGBoost\") #, plot_chance_level=True)\n",
    "    \n",
    "_ = display.ax_.set_title(\"2-class Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*y_pred, sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8f971b8dfd9a50933f9f90bf438ea60c1e2a4d59ca31e1c83263c2e44374f38"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('linapd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
